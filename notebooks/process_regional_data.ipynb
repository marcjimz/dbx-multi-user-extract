{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68a26448-852e-4ba9-95f2-212312330a3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# This notebook processes regional data based on the executing user's permissions\n",
    "\n",
    "# Get parameters\n",
    "dbutils.widgets.text(\"catalog\", \"security_demo\", \"Catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"orders_data\", \"Schema\")\n",
    "\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "\n",
    "# Show current user context\n",
    "print(f\"Executing as: {spark.sql('SELECT CURRENT_USER()').collect()[0][0]}\")\n",
    "print(f\"Compute Type: Serverless\")\n",
    "\n",
    "# Query 1: Count accessible records\n",
    "customer_count = spark.sql(f'''\n",
    "    SELECT COUNT(*) as count \n",
    "    FROM {catalog}.{schema}.customers\n",
    "''').collect()[0][0]\n",
    "\n",
    "order_count = 0\n",
    "try:\n",
    "    order_count = spark.sql(f'''\n",
    "        SELECT COUNT(*) as count \n",
    "        FROM {catalog}.{schema}.orders\n",
    "    ''').collect()[0][0]\n",
    "except Exception as e:\n",
    "    print(f\"Note: Cannot access orders table - {str(e)[:50]}...\")\n",
    "\n",
    "print(f\"Accessible customers: {customer_count}\")\n",
    "print(f\"Accessible orders: {order_count}\")\n",
    "\n",
    "# Query 2: Show regional breakdown\n",
    "regional_summary = spark.sql(f'''\n",
    "    SELECT \n",
    "        region,\n",
    "        COUNT(*) as customer_count,\n",
    "        COUNT(DISTINCT customer_segment) as segments\n",
    "    FROM {catalog}.{schema}.customers\n",
    "    GROUP BY region\n",
    "    ORDER BY region\n",
    "''')\n",
    "\n",
    "print(\"\\\\nRegional Summary:\")\n",
    "regional_summary.show()\n",
    "\n",
    "# Query 3: Test column masking\n",
    "masked_data = spark.sql(f'''\n",
    "    SELECT \n",
    "        customer_name,\n",
    "        email,\n",
    "        phone,\n",
    "        region\n",
    "    FROM {catalog}.{schema}.customers\n",
    "    LIMIT 3\n",
    "''')\n",
    "\n",
    "print(\"\\\\nSample Customer Data (with masking):\")\n",
    "masked_data.show(truncate=False)\n",
    "\n",
    "# Return results as JSON for verification\n",
    "import json\n",
    "results = {\n",
    "    \"user\": spark.sql('SELECT CURRENT_USER()').collect()[0][0],\n",
    "    \"customer_count\": customer_count,\n",
    "    \"order_count\": order_count,\n",
    "    \"regions\": [row.region for row in regional_summary.collect()],\n",
    "    \"compute\": \"serverless\"\n",
    "}\n",
    "\n",
    "dbutils.notebook.exit(json.dumps(results))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "process_regional_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
